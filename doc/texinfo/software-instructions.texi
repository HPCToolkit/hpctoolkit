\input texinfo  @c -*-texinfo-*-

@setfilename install.texi
@settitle Installing HPCToolkit with Spack

@ifinfo
Installing HPCToolkit with Spack@*
================================
@end ifinfo

@contents

@ifinfo
@w{ }
@end ifinfo

@section Introduction

These notes describe how to build and install HPCToolkit and hpcviewer
and their prerequisites with Spack.  HPCToolkit proper (hpcrun,
hpcstruct and hpcprof) is used to measure and analyze an application's
performance and then produce a database for hpcviewer.  HPCToolkit is
supported on the following platforms.  IBM Blue Gene is no longer
supported.

@enumerate
@item
Linux (64-bit) on x86_64, little-endian powerpc (power8 and 9) and ARM
(aarch64).  Big endian powerpc is no longer supported.

@item
Cray on x86_64 and Compute Node Linux.
@end enumerate

@noindent
We provide binary distributions for hpcviewer and hpctraceviewer on
Linux (x86_64, ppc64/le and aarch64), Windows (x86_64) and MacOS
(x86_64, M1 and M2).  HPCToolkit databases are platform-independent and it
is common to run hpcrun on one machine and then view the results on
another machine.

We build HPCToolkit and its prerequisite libraries from source.
HPCToolkit has some 15-20 base prerequisites (more for cuda or rocm)
and we now use spack to build them.  It is possible to use spack to
install all of hpctoolkit or build just the prerequisites and then
build hpctoolkit with the traditional @code{configure ; make ; make
install} method from autotools.  Developers will probably want to run
@code{configure} and @code{make} manually, but both methods are
supported.

Note: the old method of using hpctoolkit-externals to build the
prerequisite libraries is now superseded by spack and is no longer
supported.

These notes are written mostly from the view of using spack to build
hpctoolkit and its dependencies.  If you are a more experienced spack
user, especially if you want to use spack to build hpctoolkit plus
several other packages, then you will want to adapt these directions to
your own needs.

Spack documentation is available at:

@example
@uref{https://spack.readthedocs.io/en/latest/index.html}
@end example

@noindent
The current status of using Spack for HPCToolkit is at:

@example
@uref{http://hpctoolkit.org/spack-issues.html}
@end example

@noindent
Last revised: February 6, 2023.

@c ------------------------------------------------------------

@section Prerequisites

Building HPCToolkit requires the following prerequisites.

@enumerate
@item
The GNU gcc, g++ and gfortran compilers version 8.x or later with
support for C++17.  On systems with older compilers, you can use spack
to build a later version of gcc.

@item
GNU glibc version 2.16 or later.  Note: Red Hat 6.x uses glibc 2.12
which is too old.

@item
Basic build tools: make, ld, ar, objcopy, nm, etc, and shell utilities:
bash, sed, awk, grep, etc.  Most Linux systems have these tools, or else
you couldn't compile anything.

@item
Cmake version 3.12 or later, perl version 5.x, and python version
3.8 or later.  On systems that are missing these tools or have versions
that are too old, you can use spack to build a later version.

@item
Git and curl for downloading files.

@item
(optional) Environment (TCL) or LUA (Lmod) modules, if you want to make
HPCToolkit available as a module.  Again, spack can install these
packages if they are missing from your system.
@end enumerate

@noindent
Hpcviewer and hpctraceviewer require Java 11 or later.  Spack can
install Java, if needed.  On Linux, the viewers also require GTK+
version 3.20 or later.

@c ------------------------------------------------------------

@section Spack Notation

Spack uses a special notation for specifying the version, variants,
compilers and dependencies when describing how to build a package.
This combination of version, variants, etc is called a 'spec' and is
used both on the command line and in config files.

@enumerate
@item
'@@' specifies the package version.  @code{spack info <package>} shows
the available versions and variants for a package.  In most cases,
spaces are optional between elements of a spec.  For example:

@example
boost@@1.77.0    dyninst @@12.1.0    hpctoolkit @@develop
@end example

@item
'+', '-', '~' specify boolean (on/off) variants.  Note: @code{-}
(dash) and @code{~} (tilde) both mean 'off'.  Use dash after a space
and tilde after a non-space.  For example:

@example
elfutils+bzip2~nls    elfutils +bzip2 -nls    elfutils@@0.186 +bzip2~nls
@end example

@item
'name=value' specifies a non-boolean variant, for example:

@example
dyninst+openmp build_type=RelWithDebInfo    xerces-c@@3.2.2 transcoder=iconv
@end example

@item
'%' specifies the build compiler and its version, for example:

@example
hpctoolkit@@develop %gcc@@8.5.0
@end example

@item
'cflags', 'cxxflags', 'fflags', 'cppflags', 'ldflags' and 'ldlibs' are
special name/value variants for compiler flags.  These are normally not
needed, but if you do need to add a flag to the build, then one example
might be:

@example
amg2013 cflags='-O2 -mavx512pf'
@end example

@item
'^' represents a dependency spec.  The spec for a dependency package is
a full spec and may include its own version, variants, etc.  For
example:

@example
hpctoolkit@@develop ^dyninst@@12.1.0+openmp
@end example

@item
'arch', 'platform', 'os' and 'target' are special options for the
system architecture and machine type.  Platform is normally 'linux',
or else 'cray' (or even 'darwin').  OS (or 'operating_system') is the
Linux distribution, 'rhel8', 'sles15', etc, and target is the machine
type, 'x86_64', 'ppc64le', etc.  Arch is a triple of platform, os and
target separated by dashes, for example @code{linux-rhel8-x86_84}.

Normally, a system has only one arch type and you don't need to
specify this.  However, for systems with separate front and back-end
types, the default is the back end.  For example, if you wanted to
build for the front end on Cray, then you could use something like
this.

@example
python@@3.7.4 arch=cray-sles15-x86_64    boost os=fe
@end example

@noindent
Now that spack has implemented microarchitecture targets
(haswell, ivybridge, etc), you can use 'target' to build for a generic
x86_64 or a specific CPU type.  For example:

@example
amg2013 target=x86_64    lulesh target=ivybridge
@end example

@noindent
You can use @code{spack arch} to display the generic, top-level
families and the micro-arch targets.

@example
spack arch --known-targets
@end example
@end enumerate

@noindent
The following command gives a summary of spack spec syntax.

@example
spack help --spec
@end example

@noindent
When writing a spec (for @code{spack spec, install}, etc), spack will
fully resolve all possible choices for the package and all of its
dependencies and create a unique hash value for that exact
configuration.  This process is called 'concretization.'  To see how
spack would concretize a spec, use @code{spack spec}.

@example
spack spec hpctoolkit@@develop ^elfutils@@0.187 ^boost@@1.77.0
@end example

@example
@uref{https://spack.readthedocs.io/en/latest/basic_usage.html#specs-dependencies}
@end example

@c ------------------------------------------------------------

@section Clone Spack and HPCToolkit

Spack is available via git clone from GitHub.  This includes the core
spack machinery and recipes for building over 6,500 packages (and
growing).  You should also clone HPCToolkit for the
@code{packages.yaml} file which is used to configure the spack build.
Note: spack is on GitHub, but hpctoolkit has moved to GitLab.

@example
git clone https://github.com/spack/spack.git
git clone https://gitlab.com/hpctoolkit/hpctoolkit.git
@end example

@noindent
After cloning, add the @code{spack/bin} directory to your PATH, or else
source the spack @code{setup-env} script.

@example
(bash)   .  /path/to/spack/share/spack/setup-env.sh
(csh)    setenv SPACK_ROOT /path/to/spack/root
         source $SPACK_ROOT/share/spack/setup-env.csh
@end example

@noindent
It suffices to add @code{spack/bin} to your PATH (or even symlink the
spack launch script).  Sourcing the @code{setup-env} script adds extra
support for modules built by spack.

@c ------------------------------------------------------------

@section Config.yaml and Install Roots

@code{config.yaml} is the top-level spack config file.  This specifies
the directory layout for installed files and the top-level spack
parameters.  There are two fields that you normally want to set,
especially if you want to install packages and modules for hpctoolkit
in a public directory.  For a module to be available, both the
@code{install_tree} and @code{module_roots} directories must be
accessible.

By default, spack installs packages inside the spack repository at
@code{spack/opt/spack}.  To use another location, set the @code{root}
field under @code{install_tree} in @code{config.yaml}.

@example
config:
  install_tree:
    root: /path/to/top-level/install/directory
@end example

@noindent
By default, spack installs module files inside the spack repository at
@code{spack/share/spack}.  The syntax for resetting this is changing.
In the new syntax, the paths go in @code{modules.yaml}.

@example
modules:
  default:
    roots:
      # normally, need only one of these
      tcl:  /path/to/top-level/tcl-module/directory
      lmod: /path/to/top-level/lmod-module/directory
    enable:
      - tcl  (or lmod)
@end example

@noindent
Note: for Lmod modules, you should also turn off autoload for
hpctoolkit.  Autoload loads the modules for hpctoolkit's dependencies.
But hpctoolkit does not need this and loading them may interfere with
an application's dependencies.

@example
modules:
  default:
    lmod:
      hpctoolkit:
        autoload: none
      all:
        autoload: direct
@end example

@noindent
There are a few other fields that you may want to set for your local
system.  These are all in @code{config.yaml}.

@enumerate
@item
@code{connect_timeout} -- some download sites, especially sourceforge
are often slow to connect.  If you find that connections are timing out,
then increase this time to 30 or 60 seconds (default is 10 seconds).

@item
@code{url_fetch_method} -- by default, spack uses a python library
(urllib) to fetch source files.  If you have trouble downloading
files, try changing this to @code{curl}.

@item
@code{build_jobs} -- by default, spack uses all available hardware
threads for parallel make, up to a limit of 16.  If you want to use a
different number, then set this.
@end enumerate

@noindent
There are also parameters for the locations of the build directories,
the cache of downloaded tar files, etc, which you may wish to set.

The default @code{config.yaml} file is in the spack repository at
@code{spack/etc/spack/defaults}.  The simplest solution is to copy this
file one directory up and then edit the copy (don't edit the default
file directly).

@example
cd spack/etc/spack
cp defaults/config.yaml .
vi config.yaml
@end example

@noindent
Alternatively, you could put this file in a separate directory, outside
of the spack repository and then use @code{-C dir} on the spack command
line.  (The @code{-C} option goes before the spack command name.)

@example
spack -C dir install ...
@end example

@noindent
Note: if you put @code{config.yaml} in @code{spack/etc/spack}, then it
will apply to every spack command for that repository (and you won't
forget).  Putting it in a separate directory is more flexible because
you can support multiple configurations from the same repository.  But
then you must use @code{-C dir} with every spack command or else you
will get inconsistent results.

You can view the current configuration and see where each entry comes
from with @code{spack config}.

@example
spack [-C dir] config get config
spack [-C dir] config blame config
@end example

@noindent
See the spack docs on `Configuration Files' and `Basic Settings'.

@example
@uref{https://spack.readthedocs.io/en/latest/configuration.html}
@uref{https://spack.readthedocs.io/en/latest/config_yaml.html}
@end example

@c ------------------------------------------------------------

@section Packages.yaml

The @code{packages.yaml} file specifies the versions and variants for
the packages that spack installs and serves as a common reference
point for HPCToolkit's prerequisites.  This file also specifies the
paths or modules for system build tools (cmake, python, etc) to avoid
rebuilding them.  Put this file in the same directory as
@code{config.yaml}.  A sample @code{packages.yaml} file is available
in the @code{spack} directory of the hpctoolkit repository.

There are two main sections to @code{packages.yaml}.  The first
specifies the versions and variants for hpctoolkit's prereqs.  By
default, spack will choose the latest version of each package (plus
any constraints from hpctoolkit's @code{package.py} file).  In most
cases, this will work, but not always.  If you need to specify a
different version or variant, then set this in @code{packages.yaml}.

Note: the versions and variants specified in hpctoolkit's
@code{package.py} file are hard constraints and should not be changed.
Variants in @code{packages.yaml} are preferences that may be modified
for your local system.  (But don't report a bug until you have first
tried the versions from @code{packages.yaml} that we supply.)

There are at least two packages with a variant that you may need to
change depending on your system.  But always check the current
@code{packages.yaml} file to see if any more have been added.

@enumerate
@item
@code{binutils} -- avoid versions 2.35 and 2.35.1, they have a bug
that spews errors from hpcprof.  Use 2.34 until 2.36 is available.

@item
@code{intel-tbb} -- for very old Intel or AMD systems that don't support
transactional memory, change @code{+tm} to @code{~tm}.  (This option has
no effect on non-x86 systems.)

@end enumerate

@subsection External Packages

The other sections in @code{packages.yaml} specify paths or modules
for other packages and system build tools.  Building hpctoolkit's
prerequisites requires cmake 3.10 or later, perl 5.x and python 3.8 or
later.  There are three ways to satisfy these requirements: a system
installed version (eg, /usr), a pre-built module or build from
scratch.

By default, spack will rebuild these from scratch, even if your local
version is perfectly fine.  If you already have an installed version
and prefer to use that instead, then you can specify this in
@code{packages.yaml}.

The easiest way to use a pre-built package is to let spack find the
package itself.  Make sure the program is on your PATH and run
@code{spack external}.  For example, to search for @code{cmake}, use:

@example
spack external find cmake
@end example

@noindent
This does not work for every spack package, but it does work with
@code{cmake}, @code{perl} and @code{python}.  Note: spack puts these
entries in @code{packages.yaml} in the @code{.spack} subdirectory of
your home directory.

You can also add these entries manually to @code{packages.yaml}.  For
example, this entry says that cmake 3.7.2 is available from module
@code{CMake/3.7.2}.  @code{buildable: False} is optional and means
that spack must find a matching external spec or else fail the build.

@example
cmake:
  externals:
  - spec: cmake@@3.7.2
    modules:
    - CMake/3.7.2
  buildable: False
@end example

@noindent
This example says that python2 and python3 are both available in
@code{/usr/bin}.  Note that the @code{prefix} entry is the parent
directory of @code{bin}, not the bin directory itself.

@example
python:
  externals:
  - spec: python@@2.7.18
    prefix: /usr
  - spec: python@@3.6.8
    prefix: /usr
@end example

@noindent
Note: as a special rule for python, use package name @code{python},
even though the program name is python2 or python3.

@noindent
@b{Warning:} It is Ok to use spack externals for build utilities that
exist on your system (cmake, perl, python).  However, we strongly
recommend that you should rebuild all prereq packages that link code
into hpctoolkit (dyninst, elfutils, etc).

@subsection Micro-Architecture Targets

Spack implements a hierarchy of micro-architecture targets, where
'target' is a specific architecture (eg, haswell, ivybridge, etc)
instead of a generic family (x86_64, ppc64le or aarch64).  This allows
the compiler to optimize code for the specific target.

You will notice this choice in two main places: the 'spack spec' and the
path for the install directory.  For example, @code{linux-rhel7-x86_64}
might become @code{linux-rhel7-broadwell}.  You can use @code{spack
arch} to see the list of generic families and micro-architecture
targets.

@example
spack arch --known-targets
@end example

@noindent
If you prefer a generic install, you can use the @code{target} option
to specify a generic family (x86_64, ppc64le or aarch64) instead of a
micro-architecture target.  This would be useful for a shared install
that needs to work across multiple machines with different micro-arch
types.  For example:

@example
spack install hpctoolkit ... target=x86_64
@end example

@noindent
You can also specify preferences for @code{target}, @code{compilers}
and @code{providers} in the @code{all:} section of
@code{packages.yaml}.  Note: these are only preferences, they can be
overridden on the command line.

@example
packages:
  all:
    target: [x86_64]
    compiler: [gcc@@9.3.0]
    providers:
      mpi: [openmpi]
@end example

@noindent
See the spack docs on 'Build Customization' and 'Specs and
Dependencies'.

@example
@uref{https://spack.readthedocs.io/en/latest/build_settings.html}
@uref{https://spack.readthedocs.io/en/latest/basic_usage.html#specs-dependencies}
@end example

@subsection Require, Reuse and Fresh

It is important to understand that specifications in
@code{packages.yaml} are only preferences, not requirements.  There
are other choices that spack ranks higher.  In particular, spack will
prefer to reuse an existing package that doesn't conform to
@code{packages.yaml} rather than rebuild a newer version.

For example, suppose you previously installed hpctoolkit with dyninst
12.1.0.  Then, some months later, you update your spack repo and want
to install a new hpctoolkit with dyninst 12.2.0.  By default, spack
will prefer to reuse the old 12.1.0 rather than rebuild the new
version.

The solution is to use @code{require:} to force spack to build the new
version.

@example
packages:
  dyninst:
    require: "@@12.2.0"
@end example

@noindent
Note:
@enumerate
@item
The value for @code{require:} is a full spec (so include @code{@@} for
version) and supersedes both version and variants.

@item
The value for @code{require:} should be a singleton spec (not a list)
and should be quoted.
@end enumerate

@noindent
By default, spack install uses @code{--reuse} which prefers reusing an
already installed package.  You can change this with @code{--fresh}
which prefers to rebuild the latest version of a package.  But
@code{--reuse} and @code{--fresh} apply to all package versions.  The
advantage of @code{require:} is that you can selectively choose the
version and variants on a package by package basis.

There are two extensions to @code{require:} that are sometimes useful.
@code{any_of} requires one or more from a list of specs, and
@code{one_of} requires exactly one from a list of specs.  For example,

@example
packages:
  boost:
    require:
      - one_of: ["@@1.75.0", "@@1.77.0"]
  elfutils:
    require:
      - any_of: ["+bzip2", "+xz"]
@end example

@noindent
You can require the target, compiler or providers in
@code{packages.yaml} as follows.  Recall that the field for
@code{require:} is a spec in quotes.

@example
packages:
  all:
    require: "%gcc@@9.3.0 target=x86_64"
  mpi:
    require: "mpich@@4.0"
@end example

@example
@uref{https://spack.readthedocs.io/en/latest/build_settings.html#package-requirements}
@end example

@c ------------------------------------------------------------

@section Bootstrapping Clingo

The 'concretizer' is the part of spack that converts a partial spec
into a full spec with values for the version and variants of every
package in the spec plus all dependencies.  The new concretizer for
spack (clingo) is a third-party python library for solving answer set
logic problems (eg, satisfiability).  Normally, this only needs to be
set up once per machine, the first time you run spack.

The easiest way to install clingo it to use spack's pre-built
libraries.  These are available for Linux (x86_64, ppc64le, aarch64)
and Macos/Darwin (x86_64) for python 3.5 or later.  The Macos version
also requires Macos 10.13 or later and the Xcode developer package
(for python and other programs).

By default, spack will automatically install (bootstrap) clingo the
first time you run a command that uses it (eg, @code{spec} or
@code{solve}).  However, if this fails or you want to verify the steps
yourself, then follow these steps.

In @code{config.yaml}, set @code{concretizer} to @code{clingo}.

@example
config:
  concretizer: clingo
@end example

@noindent
Spack needs at least one compiler configured (see below).  If this is
your first time running spack on this machine, then use @code{compiler
find} to detect a compiler.  Finally, use @code{spack solve} to
trigger bootstrapping.

@example
spack compiler list    (to display known compilers)
spack compiler find    (to add a compiler, if needed)
spack solve zlib
==> Bootstrapping clingo from pre-built binaries
...
zlib@@1.2.11%gcc@@8.4.1+optimize+pic+shared arch=linux-rhel8-zen
@end example

@noindent
Spack stores the clingo bootstrap files in @code{~/.spack/bootstrap}.
You can check on the status of these files or clean (reset) them with
the @code{find} or @code{clean} commands.

@example
spack find -b     (displays the status of the bootstrap files)
spack clean -b    (erases the current bootstrap files)
@end example

@noindent
If the binary bootstrap fails, then try the @code{solve} step with
debugging turned on.

@example
spack -d solve zlib
@end example

@noindent
If the binary bootstrap fails or if your system is not supported, then
you will need to let spack build clingo from source.  Reset
@code{spack-install} to true and rerun @code{spack solve zlib}.  This
requires a compiler with support for C++14 and takes maybe 30-45
minutes to install all the packages.

@example
@uref{https://spack.readthedocs.io/en/latest/getting_started.html#bootstrapping-clingo}
@uref{https://github.com/alalazo/spack-bootstrap-mirrors#supported-platforms}
@end example

@c ------------------------------------------------------------

@section Compilers and compilers.yaml

Building HPCToolkit requires GNU gcc/g++ version 8.x or later with
C++17 support.  By default, spack uses the latest available version of
gcc, but you can specify a different compiler, if one is available.

Spack uses a separate file, @code{compilers.yaml} to store information
about available compilers.  This file is normally in your home directory
at @code{~/.spack/platform} where `platform' is normally `linux' (or
else `cray' or `bgq').

The first time you use spack, or after adding a new compiler, you should
run @code{spack compiler find} to have spack search your system for
available compilers.  If a compiler is provided as a module, then you
should load the module before running @code{find}.  Normally, you only
need to run @code{find} once, unless you want to add or delete a
compiler.  You can also run @code{spack compiler list} and @code{spack
compiler info} to see what compilers spack knows about.

For example, on one power8 system running RedHat 7.3, /usr/bin/gcc is
version 4.8.5, but gcc 8.3.0 is available as module @code{GCC/8.3.0}.

@example
module load GCC/8.3.0

spack compiler find
==> Added 2 new compilers to /home/krentel/.spack/linux/compilers.yaml
    gcc@@8.3.0  gcc@@4.8.5
==> Compilers are defined in the following files:
    /home/krentel/.spack/linux/compilers.yaml

spack compiler list
==> Available compilers
-- gcc rhel7-ppc64le --------------------------------------------
gcc@@8.3.0  gcc@@4.8.5

spack compiler info gcc@@8.3
gcc@@8.3.0:
    paths:
        cc = /opt/apps/software/Core/GCCcore/8.3.0/bin/gcc
        cxx = /opt/apps/software/Core/GCCcore/8.3.0/bin/g++
        f77 = /opt/apps/software/Core/GCCcore/8.3.0/bin/gfortran
        fc = /opt/apps/software/Core/GCCcore/8.3.0/bin/gfortran
    modules  = ['GCC/8.3.0']
    operating system  = rhel7
@end example

@noindent
Note: for compilers from modules, spack does not fill in the
@code{modules:} field in the @code{compilers.yaml} file.  You need to
do this manually.  In the above example, after running @code{find}, I
edited @code{compilers.yaml} to add @code{GCC/8.3.0} to the
@code{modules:} field as below.  This is important to how spack
manipulates the build environment.

@example
- compiler:
    modules: [GCC/8.3.0]
    operating_system: rhel7
    spec: gcc@@8.3.0
    ...
@end example

@noindent
Spack uses @code{%} syntax to specify the build compiler and @code{@@}
syntax to specify the version.  For example, suppose you had gcc
versions 8.5.0, 9.3.0 and 10.2.0 available and you wanted to use 9.3.0.
You could write this as:

@example
spack install package %gcc@@9.3.0
@end example

@noindent
You can also set the choice of compiler in the @code{all:} section of
@code{packages.yaml}.

@example
packages:
  all:
    compiler: [gcc@@9.3.0]
@end example

@noindent
See the spack docs on `Compiler Configuration'.

@example
@uref{https://spack.readthedocs.io/en/latest/getting_started.html#compiler-configuration}
@end example

@c ------------------------------------------------------------

@section Python

Spack uses Python for two things.  First, to run the Spack scripts
written in Python, and second, to use as a dependency for other spack
packages.  These do not have to be the same python version or install.

Currently, Spack requires at a minimum Python 3.6 to run spack at all.
But 3.6 is deprecated and support for both 3.6 and 3.7 will be removed
in a few months.  So, the best thing to do is to upgrade to Python 3.8
or later now.

If python 3.8 or later is not available on your system, then your
options to install it are: (1) load a module for a later version, (2)
use yum or apt to install a later version, (3) ask your sysadmin to
install a later version, or (4) as a last resort, compile a later
version.

If a later python is available on your system but not first in your
PATH or under a different name, you can set the environment variable
@code{SPACK_PYTHON} to the python3 binary.  For example, suppose
@code{/usr/bin/python3} is too old, but python 3.8 is available as
@code{/usr/bin/python3.8}, then you could use:

@example
export SPACK_PYTHON=/usr/bin/python3.8
@end example

@noindent
If set, @code{SPACK_PYTHON} is the path to the Python interpreter used
to run Spack.

@c ------------------------------------------------------------

@section Spack Install

First, set up your @code{config.yaml}, @code{packages.yaml} and
@code{compilers.yaml} files as above and edit them for your system.
You can see how spack will build hpctoolkit with @code{spack spec}.

@example
spack spec hpctoolkit
@end example

@noindent
Then, the ``one button'' method uses spack to install everything.

@example
spack install hpctoolkit
@end example

@noindent
@b{Tip:} Spack fetch is somewhat fragile and often has transient
problems downloading files.  You can use @code{spack fetch -D} to
pre-fetch all of the tar files and resolve any downloading problems
before starting the full install.

@example
spack fetch -D hpctoolkit
@end example

@c ------------------------------------------------------------

@section Manual Install

The manual method uses spack to build hpctoolkit's prerequisites and
then uses the traditional autotools @code{configure && make && make
install} to install hpctoolkit.  This method is primarily for
developers who want to compile hpctoolkit, edit the source code and
recompile, etc.  This method is also useful if you need some configure
option that is not available through spack.

First, use spack to build hpctoolkit's prerequisites as above.  You
can either build some version of hpctoolkit as before (which will pull
in the prerequisites), or else use @code{--only dependencies} to avoid
building hpctoolkit itself.

@example
spack install --only dependencies hpctoolkit
@end example

@noindent
Then, configure and build hpctoolkit as follows.  Hpctoolkit uses
automake and so allows for parallel make.

@example
configure  \
   --prefix=/path/to/hpctoolkit/install/prefix  \
   --with-spack=/path/to/spack/install_tree/linux-fedora26-x86_64/gcc-7.3.1  \
   ...
make -j <num>
make install
@end example

@noindent
The argument to @code{--with-spack} should be the directory containing
all of the individual package directories, normally two directories
down from the top-level @code{install_tree} and named by the platform
and compiler.  This option replaces the old @code{--with-externals}.
The following are other options that may be useful.  For the full list
of options, see @code{configure -h}.

@enumerate
@item
@code{--enable-all-static} -- build hpcprof-mpi statically linked for
the compute nodes.

@item
@code{--enable-develop} -- compile with optimization turned off for
debugging.

@item
@code{--with-package=path} -- specify the install prefix for some
prerequisite package, mostly for developers who want to use a custom,
non-spack version of some package.

@item
@code{MPICXX=compiler} -- specify the MPI C++ compiler for
hpcprof-mpi, may be a compiler name or full path.
@end enumerate

@noindent
Note: if your spack install tree has multiple versions or variants for
the same package, then @code{--with-spack} will select the one with
the most recent directory time stamp (and issue a warning).  If this
is not what you want, then you will need to specify the correct
version with a @code{--with-package} option.

@c ------------------------------------------------------------

@section Advanced Options

@subsection CUDA

Beginning with the 2020.03.01 version, HPCToolkit now supports
profiling CUDA binaries (nVidia only).  For best results, use CUDA
version 10.1 or later and Dyninst 10.1 or later.  Note: in addition to
a CUDA installation, you also need the CUDA system drivers installed.
This normally requires root access and is outside the scope of spack.

For a spack install with CUDA, use the @code{+cuda} variant.

@example
spack install hpctoolkit +cuda
@end example

@noindent
For a manual install, either download and install CUDA or use an
existing module, and then use the @code{--with-cuda} configure option.

@example
configure  \
   --prefix=/path/to/hpctoolkit/install/prefix  \
   --with-spack=/path/to/spack/install/dir   \
   --with-cuda=/path/to/cuda/install/prefix  \
   ...
@end example

@noindent
If you installed CUDA with spack in the same directory as the rest of
the prerequisites, then the @code{--with-spack} option should find it
automatically (but check the summary at the end of the configure
output).  If you are using CUDA from a separate system module, then you
will need the @code{--with-cuda} option.

@c ------------------------------------------------------------

@subsection Level Zero

HPCToolkit supports profiling Intel GPUs through the Intel Level Zero
and Intel GTPin interfaces.  For basic support (start and stop times
for GPU kernels) add the @code{+level_zero} variant.  For advanced
support inside the GPU kernel, also add the @code{+gtpin} variant.
But we recommend always compiling with gtpin and then deciding at
runtime which options to use.

@example
spack install hpctoolkit +level_zero +gtpin
@end example

@noindent
GTPin requires the @code{oneapi-igc} package which is an external only
spack package, normally installed in @code{/usr}.  You should add this
with a spack externals and let spack build the rest.  For example:

@example
packages:
  oneapi-igc:
    externals:
     - spec: oneapi-igc@@1.0.10409
       prefix: /usr
@end example

@noindent
For an autotools build, use the options:

@example
configure  \
    --with-level0=/path/to/oneapi-level-zero/prefix  \
    --with-gtpin=/path/to/intel-gtpin/prefix  \
    --with-igc=/usr   (or oneapi-igc prefix)  \
    ...
@end example

@c ------------------------------------------------------------

@subsection ROCM

HPCToolkit supports profiling AMD GPU binaries through the HIP/ROCM
interface, and beginning with version 2022.04.15, we support building
hpctoolkit plus rocm with a fully integrated spack build.  We require
ROCM 4.5.x or later (prefer 5.x), and the ROCM version should match
the version the application uses.  This is all very fluid and subject
to change.

There are two ways to build HPCToolkit plus ROCM with spack.
HPCToolkit uses four ROCM prerequisites (hip, hsa-rocr-dev,
roctracer-dev and rocprofiler-dev).  If you have AMD's all-in-one ROCM
package installed in @code{/opt}, then specify all four prereqs in
@code{packages.yaml}.  For example, if ROCM 5.0.0 is installed at
@code{/opt/rocm-5.0.0}, then you would use:

@example
packages:
  hip:
    externals:
    - spec: hip@@5.0.0
      prefix: /opt/rocm-5.0.0

  hsa-rocr-dev:
    externals:
    - spec: hsa-rocr-dev@@5.0.0
      prefix: /opt/rocm-5.0.0

  roctracer-dev:
    externals:
    - spec: roctracer-dev@@5.0.0
      prefix: /opt/rocm-5.0.0

  rocprofiler-dev:
    externals:
    - spec: rocprofiler-dev@@5.0.0
      prefix: /opt/rocm-5.0.0
@end example

@noindent
Currently, with AMD's directory layout, the hip and hsa-rocr-dev
prefixes could be specified either as @code{/opt/rocm-5.0.0} or
@code{/opt/rocm-5.0.0/hip} (and @code{/opt/rocm-5.0.0/hsa}).  But
roctracer-dev and rocprofiler-dev require @code{/opt/rocm-5.0.0}.
Also, the rocm packages do not support @code{spack external find}.
But all this is fluid and subject to change.

Alternatively, if ROCM is not installed in @code{/opt/rocm}, or if you
want to build a different version, then omit the externals definitions
in @code{packages.yaml} (but be prepared for spack to build an extra
80-90 packages).  In either case, install hpctoolkit with:

@example
spack install hpctoolkit +rocm ...
@end example

@noindent
For developers building with autotools, use the following configure
options.  If @code{/opt/rocm} is available, then use the
@code{--with-rocm} option.  Otherwise, use the other four options.

@example
configure  \
   --with-rocm=/opt/rocm \    (for all-in-one /opt/rocm)

   --with-rocm-hip=/path/to/hip/prefix  \
   --with-rocm-hsa=/path/to/hsa-rocr-dev/prefix  \
   --with-rocm-tracer=/path/to/roctracer-dev/prefix  \
   --with-rocm-profiler=/path/to/rocprofiler-dev/prefix  \
   ...
@end example

@noindent
It is allowed to mix the all-in-one option with the individual
packages.  The rule is that the specific overrides the general.

@c ------------------------------------------------------------

@subsection OpenCL

For all three GPU types, an application can access the GPU through the
native interface (CUDA, ROCM, Level Zero) or through the OpenCL
interface.  To add support for OpenCL, add the @code{+opencl} variant
in addition to the native interface.  We recommend adding opencl
support for all GPU types.  For example, with CUDA:

@example
spack install hpctoolkit +cuda +opencl
@end example

@noindent
For an autotools build, use the @code{--with-opencl} option.

@example
configure  \
    --with-cuda=/path/to/cuda/prefix  \
    --with-opencl=/path/to/opencl-c-headers/prefix  \
    ...
@end example

@c ------------------------------------------------------------

@subsection MPI

HPCToolkit always supports profiling MPI applications.  For
hpctoolkit, the spack variant @code{+mpi} is for building hpcprof-mpi,
the MPI version of hpcprof.  If you want to build hpcprof-mpi, then
you need to supply an installation of MPI.

@example
spack install hpctoolkit +mpi
@end example

@noindent
Normally, for systems with compute nodes, you should use an existing
MPI module that was built for the correct interconnect for your system
and add this to @code{packages.yaml}.  The MPI module should be built
with the same version of GNU gcc/g++ used to build hpctoolkit (to keep
the C++ libraries in sync).  For example,

@example
packages:
  mpich:
    externals:
    - spec: mpich@@4.0
      modules:
      - mpich/4.0
@end example

@subsection PAPI vs Perfmon

HPCToolkit can access the Hardware Performance Counters with either
PAPI or Perfmon (libpfm4).  By default, the hpctoolkit package uses
perfmon.  If you want to use PAPI instead, then build hpctoolkit with
@code{+papi}.  However, you can't use both due to a potential conflict
in their header files.

PAPI runs on top of the perfmon library, but PAPI uses its own,
internal copy of perfmon.  Prior to version 5.6.0, PAPI did not
install the perfmon header files, so it was impossible to access the
perfmon events through PAPI.

However, starting with version 5.6.0, PAPI now installs both the
perfmon library and its header files.  Hpctoolkit configure will
automatically detect this, so if you build hpctoolkit with a recent
enough version of PAPI, then both the PAPI and perfmon interfaces will
be available.

@c ------------------------------------------------------------

@subsection Python

Beginning with the 2023 release, HPCToolkit can now profile Python
scripts and attribute samples to python source functions instead of
the python interpreter.  This requires Python 3.10 or later and is not
the same python to run the spack scripts.  This should be the same
python used to run the application.

@example
spack install hpctoolkit +python
@end example

@noindent
When building with autotools, use the @code{--enable-python} argument
with the path to the @code{python-config} command.

@example
configure  \
    --enable-python=/path/to/python-config
    ...
@end example

@c ------------------------------------------------------------

@section Platform Specific Notes

@subsection Cray

There are two ways to build @code{hpcprof-mpi} on Cray systems
depending on how old the system is and what MPI wrapper is available.
Newer Crays have an @code{mpicxx} wrapper from the @code{cray-mpich}
module (but it may not be on your PATH).  Older Crays use the
@code{CC} wrapper from the @code{craype} module.

On either type of system, start by switching to the @code{PrgEnv-gnu}
module and unload the Darshan module if it exists.  Darshan is a
profiling tool that monitors an application's use of I/O, but it
conflicts with hpctoolkit.

@example
module swap PrgEnv-cray PrgEnv-gnu
module unload darshan
@end example

@noindent
Next, we need the front-end GCC compiler that is compatible with the
MPI compiler.  The gcc compiler should use the front-end operating
system type (sles, not cnl) and should be version 8.x or later
(preferably 9.x or later).  The @code{cc} and @code{cxx} compilers
should be gcc and g++, not the cc and CC wrappers, and the modules
should include at least @code{PrgEnv-gnu} and @code{gcc}.

For example, I have the following on Crusher at ORNL in my
@code{compilers.yaml} file (your versions may differ).  Note that
spack may report the front-end arch type as either cray or linux.

@example
compilers:
- compiler:
    spec: gcc@@11.2.0
    paths:
      cc:  /opt/cray/pe/gcc/11.2.0/bin/gcc
      cxx: /opt/cray/pe/gcc/11.2.0/bin/g++
      f77: /opt/cray/pe/gcc/11.2.0/bin/gfortran
      fc:  /opt/cray/pe/gcc/11.2.0/bin/gfortran
    modules:
      - PrgEnv-gnu/8.3.3
      - gcc/11.2.0
    operating_system: sles15
    target: x86_64
    ...
@end example

@noindent
@b{New Cray} The preferred method for newer Crays is using the
@code{+mpi} option and the @code{cray-mpich} module.  This requires
the @code{mpicxx} wrapper, although it won't be on your PATH.  Look in
the @code{$MPICH_DIR} or @code{$CRAY_MPICH_DIR} directory for the
@code{mpicxx} wrapper.  For example on Crusher, this is at the
following path, your path may be different.

@example
/opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/bin/mpicxx
@end example

@noindent
If this is available, then add a spack externals entry for
@code{cray-mpich} and the @code{mpi} virtual package to
@code{packages.yaml}.  For example, I used this entry on Crusher, your
versions may be different (put the specs in quotes).

@example
packages:
  mpi:
    require: "cray-mpich@@8.1.17"

  cray-mpich:
    externals:
      - spec: "cray-mpich@@8.1.17"
        prefix: /opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1
        modules:
         - cray-mpich/8.1.17
@end example

@noindent
Then, build with @code{+mpi} for the front-end arch type (with arch or
os).  If the front and back-end arch types are the same, then you
don't need to specify that.  For example,

@example
spack install hpctoolkit +mpi os=fe    (or arch=cray-sles15-x86_64)
@end example

@noindent
Cray's use of modules is complex and requires several modules to be
loaded at compile time.  You will likely find that the above recipe
fails with an undefined reference to one or more modules.  For
example,

@example
/usr/bin/ld: warning: libfabric.so.1, needed by /opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/lib/libmpi_gnu_91.so,
not found (try using -rpath or -rpath-link)
/usr/bin/ld: /opt/cray/pe/mpich/8.1.17/ofi/gnu/9.1/lib/libmpi_gnu_91.so:
undefined reference to `fi_strerror@@FABRIC_1.0'
@end example

@noindent
There are two solutions.  One, you could search the failing build log
to identify the missing modules and add them to the compiler entry.
This may require several modules.  For example on Crusher, I added
these modules to the compiler entry and then the build succeeded.

@example
modules:
  - PrgEnv-gnu/8.3.3
  - gcc/11.2.0
  - craype/2.7.16
  - cray-mpich/8.1.17
  - libfabric/1.15.0.0
@end example

@noindent
The other solution is to load the @code{PrgEnv-gnu} and related
modules and then install hpctoolkit with the @code{--dirty} flag.
Note: only the final hpctoolkit package needs @code{--dirty}.  For
example,

@example
spack install --only dependencies hpctoolkit +mpi os=fe
spack install --dirty hpctoolkit +mpi os=fe
@end example

@noindent
@b{Note:} Some very new Cray systems (eg, Sunspot at ANL) have
@code{PrgEnv-gnu} but use a different MPI module than
@code{cray-mpich}.  On such a system, continue to add the extra
modules to the compilers entry but use a spack externals entry for the
other MPI module.

@noindent
@b{Old Cray}
Some older Cray systems (eg, Theta at ANL) don't have the
@code{mpicxx} wrapper.  In this case, it's necessary to use the
@code{+cray} option.  This option tells hpctoolkit's configure to
search for the older @code{CC} wrapper.

Prepare the @code{PrgEnv-gnu} and GCC compiler the same as with a
newer Cray, then build hpctoolkit with the @code{+cray} option again
for the front-end arch type.

@example
spack install hpctoolkit +cray os=fe
@end example

@noindent
As with new Crays, you will likely need to add extra modules to the
compiler entry or else build with @code{--dirty}.

@noindent
@b{Autotools} For building with autotools, use the @code{MPICXX}
configure variable to specify the MPI compiler, either CC or mpicxx.
Note: the @code{--enable-all-static} option is no longer used.

@example
configure  \
    --prefix=/path/to/install/prefix  \
    --with-spack=/path/to/linux-sles15-x86_64/gcc-11.2.0  \
    MPICXX=CC    (or /path/to/mpicxx)
@end example

@noindent
@b{Old HPCToolkit Versions}
As of October 2022, we now always build @code{hpcprof-mpi} on Cray
dynamically, and the @code{+all-static} option no longer exists.
However, for old versions of hpctoolkit up to 2022.05.15, it is
possible to build @code{hpcprof-mpi} either statically or dynamically,
depending on what your system supports.  (Hpcprof-mpi is disabled for
the 2022.10.01 release.)

If your Cray supports it and @code{CC} builds static binaries, then
you can build @code{hpcprof-mpi} statically with the @code{+cray} and
@code{+cray-static} options.

@example
spack install hpctoolkit @@2022.05.15 +cray +cray-static os=fe
@end example

@noindent
The @code{+cray-static} option only applies with @code{+cray} (using
the CC wrapper) and only for versions up to 2022.05.15.

@c ------------------------------------------------------------

@section HPCToolkit GUI Interface (Hpcviewer)

Since the 2020.12 release, the HPCToolkit GUI interface provides both
profile and trace views in a single application, i.e. hpcviewer. Prior
to that, each view was a separate program: hpcviewer to analyze the
profile database, and hpctraceviewer to display the traces.

We provide binary distributions for hpcviewer on Linux (x86_64,
ppc64le and aarch64), Windows (x86_64) and MacOS (x86_64, M1 and M2).
HPCToolkit databases are platform-independent and it is common to run
hpcrun on one machine and then view the results on another machine.

Starting with 2021.01, the viewer now requires Java 11 or later,
plus GTK+ 3.20 or later on Linux.  Older viewers, up through
2020.12, require Java 8 (not 9 or later), plus GTK+ 2.x for Linux.

@subsection Spack Install

The spack install is available on Linux x86_64, little-endian ppc64le
(power8 and 9) and aarch64 ARM, and also MacOS on x86_64.  This
installs hpcviewer and includes the Java prerequisite.

For the current viewers, use openjdk with the most recent version of
Java 11 for all platforms.  Currently, this is the default, but if not,
then you can add an explicit openjdk dependency.  You can check this
with @code{spack info} and @code{spack spec}.

@example
spack info openjdk
spack install hpcviewer
spack install hpcviewer ^openjdk @@11.0.12_7    (if needed)
@end example

@noindent
For older viewers, up through 2020.12, use the latest version of
openjdk Java 8 (version 1.8).  For example (the versions may change):

@example
spack install hpcviewer ^openjdk @@1.8.0_265-b01
@end example

@noindent
Note: to run the viewer on Macos, you can either open the Finder and
click your way to the @code{hpcviewer.app} directory and double-click
on the hpcviewer icon, or else use @code{spack load hpcviewer} to put
@code{hpcviewer} on your PATH.

@subsection Manual Install

Binary distributions of the viewers for all supported platforms are
available at:

@example
@uref{http://hpctoolkit.org/download.html}
@end example

@noindent
On Linux, download the @code{linux.gtk} version of hpcviewer (and also
hpctraceviewer for older versions), unpack the tar files and run the
install scripts (for both viewers) with the path to the desired install
prefix.

@example
./install /path/to/install/directory
@end example

@noindent
On Windows and MacOS, download the @code{win32} or @code{macosx.cocoa}
versions and unpack the zip or dmg files in the desired directory.
Due to Apple's security precautions, on MacOS, you may need to use
curl or wget instead of a web browser.

Note: the manual install uses the existing system version of Java (or
one of several versions with modules), whereas the spack install
includes the java prerequisite.  That is, the spack install is
self-contained and does not need to change the system java.

@c ------------------------------------------------------------

@section Building a New Compiler

Some systems may have compilers that are too old for building
HPCToolkit or other packages.  For example, RedHat 7.x comes with gcc
4.8.x which is very old.  If your system doesn't already have modules
for later compilers, then you may need to build a new compiler
yourself.

First, pick a directory in which to install the modules and make
subdirectories for the spack packages and module files.  In this
example, I'm using @code{/opt/spack/Modules} as my top-level directory
and subdirectories @code{packages} and @code{modules}.  Edit
@code{config.yaml} and @code{modules.yaml} to add these paths.

@example
config:
  install_tree:
    root: /opt/spack/Modules/packages

modules:
  default:
    roots:
      # just use one of these
      module_roots:
        tcl:  /opt/spack/Modules/modules
        lmod: /opt/spack/Modules/modules
@end example

@noindent
Determine if your system uses TCL (environment) or Lmod modules.
Normally, the @code{module} command is a shell function.  TCL modules
use @code{modulecmd} and Lmod modules eval @code{LMOD_CMD}.  Edit
@code{modules.yaml} to enable the module type for your system.  Again,
you only need one of these (and don't use dotkit unless you work at
LLNL).

@example
modules:
  default:
    enable:
     - tcl
     - lmod
@end example

@noindent
Then, choose a version of gcc and use the default compiler (normally
@code{/usr/bin/gcc}) to build the newer version.  Currently, gcc 8.x
is a good choice that builds robustly, has enough modern features but
is not too new to cause problems for some packages.  For example,

@example
spack install gcc@@8.4.0
@end example

@noindent
Note: it is not necessary to rebuild the new compiler with itself.

@subsection Using the New Compiler

After building a new compiler, then you need to tell spack how to find
it.  First, use @code{module use} and @code{module load} to load the
module.  For TCL modules, the module files are in a subdirectory of
@code{module_roots} named after the system architecture.  For example,

@example
module use /opt/spack/Modules/modules/linux-rhel7-x86_64
module load gcc-8.4.0-gcc-4.8.5-qemsqrc
@end example

@noindent
For Lmod modules, the module directory is one level below that and the
module names are a little different.

@example
module use /opt/spack/Modules/modules/linux-rhel7-x86_64/Core
module load gcc/8.4.0-dan4vbm
@end example

@noindent
For both TCL and Lmod modules, it's best to put the @code{module use}
command in your shell's startup scripts so that @code{module avail} and
@code{module load} will know where to find them.  After loading the
module, run @code{spack compiler find}.

@example
$ spack compiler find
==> Added 1 new compiler to /home/krentel/.spack/linux/compilers.yaml
    gcc@@8.4.0
@end example

@noindent
Finally, always check the new entry in @code{compilers.yaml} and add the
name of the module to the @code{modules:} field.

@example
- compiler:
    environment: @{@}
    extra_rpaths: []
    flags: @{@}
    modules:
    - gcc-8.4.0-gcc-4.8.5-qemsqrc
    operating_system: rhel7
    paths:
      cc:  /opt/spack/Modules/packages/linux-rhel7-x86_64/gcc-4.8.5/gcc-8.4.0-qemsqrcwkk52f6neef4kg5wvoucsroif/bin/gcc
      cxx: /opt/spack/Modules/packages/linux-rhel7-x86_64/gcc-4.8.5/gcc-8.4.0-qemsqrcwkk52f6neef4kg5wvoucsroif/bin/g++
      f77: /opt/spack/Modules/packages/linux-rhel7-x86_64/gcc-4.8.5/gcc-8.4.0-qemsqrcwkk52f6neef4kg5wvoucsroif/bin/gfortran
      fc:  /opt/spack/Modules/packages/linux-rhel7-x86_64/gcc-4.8.5/gcc-8.4.0-qemsqrcwkk52f6neef4kg5wvoucsroif/bin/gfortran
    spec: gcc@@8.4.0
    target: x86_64
@end example

@noindent
Note: as long as the spack packages and modules directories remain
intact and you don't remove the @code{compilers.yaml} entry, then this
compiler will always be available from within spack.  You can also use
this compiler outside of spack by using @code{module load}.  If you
want to make this your default compiler for all spack builds, then you
can specify this in @code{packages.yaml}.  For example,

@example
packages:
  all:
    compiler: [gcc@@8.4.0]
@end example

@noindent
Also, when using the compiler from within spack, it doesn't matter if
you have the module loaded or not.  Spack will erase your environment
and re-add the appropriate modules automatically.

@example
@uref{https://spack.readthedocs.io/en/latest/getting_started.html#compiler-configuration}
@end example

@subsection Bootstrapping Environment Modules

If your system does not support modules, then you will have to add it.
If you have root access, the easiest solution is to install a system
package for modules.  If not, then use spack to install the
environment-modules package.  Source the bash or csh script in the
@code{init} directory to add the @code{module} function to your
environment.  For example,

@example
spack install environment-modules
cd /path/to/environment-modules-4.3.1-ism7cdy4xverxywj27jvjstqwk5oxe2v/init
(bash)  . ./bash
(csh)   source ./csh
@end example

@noindent
Again, add the setup command to your shell's startup scripts.

@c ------------------------------------------------------------

@section Spack Mirrors

A spack mirror allows you to download and save a source tar file in
advance.  This is useful if your system is behind a firewall, or if you
need to manually agree to a license, or if you just don't want to keep
downloading the same file over and over.

A mirror has a simple directory structure and is easy to set up.  Create
a top-level directory with subdirectories named after the spack packages
and copy the tar files into their package's subdirectory.  For example,

@example
my-mirror/
   boost/
      boost-1.66.0.tar.bz2      (from boost_1_66_0.tar.bz2)
      boost-1.70.0.tar.bz2
   dyninst/
      dyninst-10.1.0.tar.gz     (from git checkout)
   ibm-java/
      ibm-java-8.0.5.30.None    (from ibm-java-sdk-8.0-5.30-ppc64le-archive.bin)
   intel-xed/
      intel-xed-2019.03.01.tar.gz    (from git checkout)
      mbuild-2019.03.01.tar.gz       (resource from git checkout)
   jdk/
      jdk-1.8.0_202.tar.gz      (from jdk-8u202-linux-x64.tar.gz)
@end example

@noindent
Note: the names of the files in the spack mirror always follow the same,
specific format, regardless of the actual name of the tar file.  Version
is the spack name for the version (from @code{spack info}), and
extension is the same extension as the tar file (@code{tar.gz},
@code{tar.bz2}, etc) or else @code{None} for other types of files.

@example
<package-name> - <version> . <extension>
@end example

@noindent
For example, the boost 1.66.0 tar file is actually named
@code{boost_1_66_0.tar.bz2} but is stored in the mirror as
@code{boost-1.66.0.tar.bz2} and @code{jdk-8u202-linux-x64.tar.gz} is
renamed to @code{jdk-1.8.0_202.tar.gz}.

For packages that use a snapshot from a git repository (tag or commit
hash), clone the repository, checkout the desired version, make a tar
file and gzip the file.  (You should exclude the @code{.git}
subdirectory.)  But note that spack refuses to use a cached file for
the head of a branch because it is a moving target.

Finally, after creating the mirror directory, add it to spack with
@code{spack mirror add}.  For example,

@example
spack mirror add my-mirror file:///home/krentel/spack/my-mirror
spack mirror list
my-mirror    file:///home/krentel/spack/mirror
@end example

@noindent
Note: by default, spack stores downloaded files inside the spack
repository at @code{spack/var/spack/cache}.  This directory is a full
spack mirror, so instead of creating a separate directory tree, you
could just copy the files into the @code{cache} directory.  This is
useful when @code{spack fetch} has trouble downloading a file.  If you
can download the file manually, or copy it from another machine, then
just rename the file as above and copy it into the spack file cache.

For more information on mirrors, see:

@example
@uref{https://spack.readthedocs.io/en/latest/mirrors.html}
@end example

@c ------------------------------------------------------------

@section Common Problems

@subsection Unable to fetch tar file

Spack is somewhat fragile for how it downloads tar files and will
often fail for transitory network problems.  This is especially true
for packages with many dependencies.  For example:

@example
==> Installing m4
==> Searching for binary cache of m4
==> No binary for m4 found: installing from source
curl: (6) Could not resolve host: ftp.wayne.edu; Name or service not known
==> Fetching https://ftpmirror.gnu.org/m4/m4-1.4.18.tar.gz
==> Fetching from https://ftpmirror.gnu.org/m4/m4-1.4.18.tar.gz failed.
==> Error: FetchError: All fetchers failed for m4-1.4.18-vorbvkcjfac43b7vuswsvnm6xe7w7or5
@end example

@noindent
There are two workarounds.  First, assuming the problem is temporary,
simply wait 10 minutes or an hour and try again.  Second, you could
manually download the file(s) by some other means and copy them to
spack's cache directory @code{spack/var/spack/cache/<package>} or to a
spack mirror.

@subsection New releases break the build

Normally, HPCToolkit should build and work correctly with the latest
version for all of its dependencies.  But sometimes a new release will
change something and break the build.  This has happened a couple times
where a new release of Boost has broken the build for Dyninst.  Or,
maybe the latest version of gcc/g++ disallows some usage and breaks the
build.

The solution is to use @code{packages.yaml} to specify an earlier
version until the rest of the code adapts to the change.

@subsection Failure to load modules

Spack is quite aggressive about compiling with a clean environment and
will unload modules unless they are specifically required by some config
file (@code{compilers.yaml} or @code{packages.yaml}).  This can result
in a situation where you think some compiler or build tool is available
from your environment but spack removes it during the build.

In this example, I am using modules for @code{GCC/8.3.0} and
@code{CMake/3.8.2}.  Spack finds the gcc 8.3.0 compiler and I added
@code{cmake@@3.8.2} to @code{packages.yaml}.  But I failed to add the
@code{modules:} field for gcc 8.3.0 in @code{compilers.yaml}.  As a
result, the build fails with:

@example
cmake: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by cmake)
cmake: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by cmake)
cmake: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by cmake)
==> Error: ProcessError: Command exited with status 1:
@end example

@noindent
The problem is that cmake 3.8.2 was built with g++ 8.3.0, but spack is
running cmake without the GCC/8.3.0 libraries and so the build fails as
above.  One way to confirm this is to rerun @code{spack install --dirty}
which then succeeds.  The @code{--dirty} option tells spack not to
unload your modules.  Whenever the build fails with a mismatched library
as above and especially when @code{--dirty} fixes the problem, this is a
clear sign that spack is missing a module during the build.

Although @code{--dirty} may make the build succeed, there should be no
case where this is necessary.  The correct solution is to fill in the
@code{modules:} field in @code{compilers.yaml} or some other config
file.  See the section on Compilers above.
@bye
